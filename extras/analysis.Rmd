---
title: "Unpacking Test Driven Development"
output: html_notebook
---

```{r include=FALSE}
# load libs
require(tidyverse)
require(lmerTest)

r2.corr.mer = function(m) {
    lmfit <-  lm(model.response(model.frame(m)) ~ fitted(m))
    summary(lmfit)$r.squared
}
```

```{r}
# Read data
all = read.csv('./all-metrics.csv')

# Project 3 was qualitatively different from others, so filter it out
p124 = all %>% filter(assignment %in% c('Project 1', 'Project 2', 'Project 4'))
```

## Testing behaviors measured

**Balance of effort on test-writing and solution-writing**:

* Project coevolution (`pc`) -- Percent of overall code in each work session that is test code
* Method coevolution (`mc`) -- Percent of method-specific code in each work session that is test code *for that method*

**Sequence or order of test-code and solution-code writing**:

* Average recency (`t.rec`) -- For each method's timeline, approximately when were changes made to relevant test code?
* Line recency (`l.rec`) -- If all changes made to a method are placed in a line, where in that sequence to changes to relevant test code fall?
* Additions before relevant test creation (`tc`) -- How much work was done between defining a method and writing the first test code for the method?

## Analysis

**Model information**

* Models are *linear mixed-effects models* -- assignments are repeated measures over subjects (students).
* Used backward stepwise selection to select the best model.

**Outcome measures**

* External quality (correctness)
* Internal quality (code coverage)

#### External quality (correctness)

**Model estimates**

```{r include=FALSE}
m.124.correctness = step(lmer(score.reftest ~ mc + tc + pc + l.rec + t.rec + (1 | userName), 
                              data = p124, REML = F), reduce.random = F)$model
r1 = r2.corr.mer(m.124.correctness) * 100
s1 = summary(m.124.correctness)
```

```{r echo=FALSE}
s1$coefficients
```

Significant contributors are *overall balance of test and solution code* (`mc`), *average line recency* (`l.rec`), and *average time recency* (`t.rec`).

These three measures explain `r round(r1, digits = 2)`% of the variance in external quality.

**Result summary in English:** 

Subjects produced *more semantically correct* programs when:

1. They maintained a balance of relevant test-code writing and solution-code writing (higher values for `mc`)
2. They wrote relevant test-code after relatively fewer changes to solution code (lower values for `l.rec`)
3. They wrote relevant test-code after relatively *more* time had passed since the writing of solution code (higher values for `t.rec`). 

I can't really explain the existence of results 2 and 3. I would need to think about it some more.

#### Internal quality (code coverage)

**Model estimates**

```{r include=FALSE}
m.124.coverage = step(lmer(elementsCovered ~ mc + tc + pc + l.rec + t.rec + (1 | userName), 
                              data = p124, REML = F), reduce.random = F)$model
r2 = r2.corr.mer(m.124.coverage) * 100
s2 = summary(m.124.coverage)
```

```{r echo=FALSE}
s2$coefficients
```

Only significant contributor is *overall balance of test and solution code* (`pc`).

This measure explains `r round(r2, digits = 2)`% of the variance in internal quality.

**Result summary in English:** 

Subjects produced test suites with *higher conditional coverage* when:

1. They maintained an overall balance of test-code writing and solution-code writing (higher values for `pc`)